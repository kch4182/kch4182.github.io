# 학번 : 2017250050
# 이름 : 조현석

# 과제 1

조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

## 1. 데이터 준비


```python
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt # from matplot import pyplot as plt 똑같은 의미
from sklearn import datasets
```


```python
iris = datasets.load_iris()

# 꽃잎의 너비 2인 붓꽃의 품종은 버지니카이다.
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 너비

# iris["target"]에서 꽃잎 너비가 2일 때만 True값 발생
y = (iris["target"] == 2).astype(np.int) 

# y를 int형으로 변환
# 버지니카는 1로 나머지는 0으로 변환
# y = y.astype(np.int)
print(y)
```

    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1]
    


```python
# 샘플에 편향 추가

X_with_bias = np.c_[np.ones([len(X), 1]), X]
# 결과를 일정하게 유지하기 위해 랜덤 시드 지정
np.random.seed(2042)

print(X_with_bias[:10])
```

    [[1.  1.4 0.2]
     [1.  1.4 0.2]
     [1.  1.3 0.2]
     [1.  1.5 0.2]
     [1.  1.4 0.2]
     [1.  1.7 0.4]
     [1.  1.4 0.3]
     [1.  1.5 0.2]
     [1.  1.4 0.2]
     [1.  1.5 0.1]]
    

## 2. 데이터셋 분할

데이터셋을 훈련, 검증, 테스트 용도로 6대 2대 2의 비율로 무작위로 분할한다.

* 훈련 세트: 60%
* 검증 세트: 20%
* 테스트 세트: 20%


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```


```python
rnd_indices = np.random.permutation(total_size) # 인덱스를 무작위로 섞는다.
```


```python
# 6:2:2로 훈련, 검증, 테스트 세트로 분할

# train dataset
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

# validation dataset
X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

# test dataset
X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]

print("feature Matrix: ", X_train.shape, X_valid.shape, X_test.shape)
print("target vector: ", y_train.shape, y_valid.shape, y_test.shape)
```

    feature Matrix:  (90, 3) (30, 3) (30, 3)
    target vector:  (90,) (30,) (30,)
    


```python
print(y_train)
```

    [0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0
     0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0
     0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0]
    

## 3. 타깃 변환
꽃 종류 0, 1, 2 -> 2나 1에서 숫자크기에 대해 우선순위, 영향을 줄 수 있기 때문에 one hot을 사용하여 범주형 데이터를 바꿔준다.


```python
def to_one_hot(y):
    n_classes = y.max() + 1                 # 클래스 수
    m = len(y)                              # 샘플 수
    Y_one_hot = np.zeros((m, n_classes))    # (샘플 수, 클래스 수) 0-벡터 생성
    Y_one_hot[np.arange(m), y] = 1          # 샘플 별로 해당 클래스의 값만 1로 변경. (넘파이 인덱싱 활용)
    return Y_one_hot
```


```python
Y_train_one_hot = to_one_hot(y_train)
Y_valid_one_hot = to_one_hot(y_valid)
Y_test_one_hot = to_one_hot(y_test)
```

## 4. 로지스틱 회귀 구현

### 4-1. 시그모이드 함수 정의

시그모이드 함수를 정의하여 경사하강법에서 사용할 수 있다.


```python
def logistic(x):
    return 1.0 / (1.0 + np.exp(-x))
```

### 4-2. 경사하강법

경사하강법을 사용하여 세타 값을 구할 수 있다.


```python
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개
```


```python
Theta = np.random.randn(n_inputs, 1) # n_inputs -> 특성, n_outputs -> 클래스
```


```python
#  배치 경사하강법 구현
eta = 0.1 # 학습률
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)    
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon) + (1 - Y_train_one_hot ) * np.log(1 - Y_proba + epsilon), axis=1))
        print(iteration, loss)       
    
    # Y_proba = np.where(Y_proba>=0.5,1,0)      # 시그모이드 함수를 사용하여 0.5보다 크면 1, 작으면 0으로 data 값 변경
    error = Y_proba - Y_train_one_hot      #  Y_proba -> 0과1, error -> 0 or -1 
    gradients = 1/m * X_train.T.dot(error) # -> 세타값 수정,  T -> 전치행렬
    
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```

    0 1.7027587596570046
    500 0.5989852865925711
    1000 0.4849055842061277
    1500 0.4288266121426535
    2000 0.3925608628462095
    2500 0.3661475166018786
    3000 0.3456019193132381
    3500 0.32894148132145823
    4000 0.3150377275718563
    4500 0.30318699641784314
    5000 0.29292076850443666
    

경사하강법을 사용하여 구한 Theta값은 다음과 같다.


```python
Theta
```




    array([[ 10.5759757 , -10.56492618],
           [ -0.53337662,   0.53611169],
           [ -4.8419295 ,   4.82694082]])



Theta값을 활용하여 Y_proba를 구한 뒤 y_predict를 사용하여 y_valid와 비교하여 정확도를 구할 수 있다.


```python
logits = X_valid.dot(Theta)              
Y_proba = logistic(logits)
y_predict = np.argmax(Y_proba, axis=1)          # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.9666666666666667



## 5. 규제가 추가된 경사하강법 활용




```python
eta = 0.08
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon) + (1 - Y_train_one_hot ) * np.log(1 - Y_proba + epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```

    0 2.4721259115384857
    500 0.7522525430412024
    1000 0.6948762090371331
    1500 0.6758876636466287
    2000 0.6680169015595655
    2500 0.6644016260086401
    3000 0.6626452212738065
    3500 0.6617625143327626
    4000 0.6613091401483169
    4500 0.6610728661353034
    5000 0.6609484954749916
    


```python
len(Y_proba)
```




    90




```python
logits = X_valid.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9333333333333333



## 6. 조기 종료 추가

모델의 훈련 세트에 대한 과대 적합 방지를 위해 훈련을 적절한 시기에 중단


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta)
    Y_proba = logistic(logits)
    xentropy_loss = -1/m*(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon) + (1 - Y_valid_one_hot ) * np.log(1 - Y_proba + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 1.8974463846026375
    500 0.29227979551382216
    621 0.2912609834871013
    622 0.29126103697852784 조기 종료!
    


```python
logits = X_valid.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9666666666666667



## 7. 전체 데이터셋 대한 예측 결과 그래프

그래프에서 노란색과, 초록색을 구분하여 노란색은 Not virginica, 초록색은 virginica이다.


```python
# (0, 8) x (0, 3.5) 크기의 직사각형 안의 모든 점을 대상으로 예측한 후에
# 예측 결과를 이용하여 색상으로 구분하고 등고선도 그리기 위한 준비작업
# 가로는 500개의 구간으로, 세로는 200개의 구간으로 쪼개짐.
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]
X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]

# 직사각형 점 대상 예측하기
logits = X_new_with_bias.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.argmax(Y_proba, axis=1)

# 등고선용 정보
zz1 = Y_proba[:, 1].reshape(x0.shape)                            # 버시컬러 기준 예측 확률
zz = y_predict.reshape(x0.shape)                                 # 예측값

# 붓꽃 샘플 그리기
plt.figure(figsize=(10, 4))
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris virginica")  # 파랑 사각형 => 버지니카ㅣ 
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Not virginica")      # 노랑 원 => 버시컬러,세토사

# 등고선 그리기
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)                       # 노랑, 청보라, 녹색 바탕색
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)              # 버시컬러 기준 예측 확률 등고선
plt.clabel(contour, inline=1, fontsize=12)

# 기타 도표 정보
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.show()
```


    
![png](output_33_0.png)
    


## 8. 데스트 세트 평가

구한 Theta 값을 사용하여 Y_proba를 구한 후 y_predict를 구할 수 있다.
구한  y_predict를 사용해 y_test랑 비교 후 정확도를 구할 수 있다.
해당 Theta를 사용하여 얻은 정확도는 0.93이 나온 것을 확인할 수 있다.

Y_proba는 다음과 같다.


```python
print(Y_proba)
```

    [[0.98028102 0.021777  ]
     [0.9800897  0.02198107]
     [0.97989655 0.02218701]
     ...
     [0.05235776 0.94459158]
     [0.05187114 0.94508856]
     [0.05138879 0.94558134]]
    


```python
logits = X_test.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9333333333333333



# 과제 2

과제 1에서 구현된 로지스틱 회귀 알고리즘에 일대다(OvR) 방식을 적용하여 붓꽃에 대한 다중 클래스 분류 알고리즘을 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.

## 1. 데이터 준비

과제2에서 필요한 모듈을 import한다.


```python
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt # from matplot import pyplot as plt 똑같은 의미
from sklearn import datasets
```


```python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이
y = iris["target"]
```


```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```


```python
np.random.seed(2042)
```

## 2. 데이터셋 분할

데이터셋을 훈련, 검증, 테스트 용도로 6대 2대 2의 비율로 무작위로 분할한다.

* 훈련 세트: 60%
* 검증 세트: 20%
* 테스트 세트: 20%


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                  # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```


```python
rnd_indices = np.random.permutation(total_size)
```


```python
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```


```python
print(y_valid)
```

    [0 2 0 0 1 2 2 0 0 0 1 1 2 2 1 1 2 1 2 1 0 1 2 2 1 0 0 1 1 2]
    

## 3. 타깃 변환


```python
y_train[:5]
```




    array([0, 1, 2, 1, 1])



one_hot을 사용하여 타깃을 변환한다.


```python
def to_one_hot(y):
    n_classes = y.max() + 1                 # 클래스 수
    m = len(y)                              # 샘플 수
    Y_one_hot = np.zeros((m, n_classes))    # (샘플 수, 클래스 수) 0-벡터 생성
    Y_one_hot[np.arange(m), y] = 1          # 샘플 별로 해당 클래스의 값만 1로 변경. (넘파이 인덱싱 활용)
    return Y_one_hot
```


```python
Y_train_one_hot = to_one_hot(y_train)
Y_valid_one_hot = to_one_hot(y_valid)
Y_test_one_hot = to_one_hot(y_test)
```

to_one_hot을 사용하여 Y_train_one_hot을 다음과 같은 형태로 작성


```python
print(Y_train_one_hot[:10])
```

    [[1. 0. 0.]
     [0. 1. 0.]
     [0. 0. 1.]
     [0. 1. 0.]
     [0. 1. 0.]
     [1. 0. 0.]
     [0. 1. 0.]
     [0. 1. 0.]
     [0. 1. 0.]
     [1. 0. 0.]]
    

## 4. 데이터 구분

Y_train_one_hot을 꽃 3가지로 분류해서 사용

* Y_A : Iris-Setosa(세토사)
* Y_B : Iris-Versicolor(버시컬러)
* Y_C : Iris-Virginica(버지니카) 



```python
# np.array 선언
Y_A_train = np.array([])
Y_B_train = np.array([])
Y_C_train = np.array([])

Y_A_valid = np.array([])
Y_B_valid = np.array([])
Y_C_valid = np.array([])

Y_A_test = np.array([])
Y_B_test = np.array([])
Y_C_test = np.array([])

# for문을 사용해서 Y_train_one_hot에서 각각의 열을 변수에 따로 넣는다.
# Y_A_train에 첫번째 행만 넣을 것이다.
for i in Y_train_one_hot :
  j = 0
  Y_A_train = np.append(Y_A_train, np.array(i[j]))

# Y_B_trian에 두번째 행만 넣을 것이다.
for i in Y_train_one_hot :
  j = 1
  Y_B_train = np.append(Y_B_train, np.array(i[j]))

# Y_C_trian에 세번째 행만 넣을 것이다.
for i in Y_train_one_hot :
  j = 2
  Y_C_train = np.append(Y_C_train, np.array(i[j]))
```

for문을 사용해서 Y_valid_one_hot에서 각각의 열을 변수에 따로 넣는다.


```python
# Y_A_valid에 첫번째 행만 넣을 것이다.
for i in Y_valid_one_hot :
  j = 0
  Y_A_valid = np.append(Y_A_valid, np.array(i[j]))

# Y_B_valid에 두번째 행만 넣을 것이다.
for i in Y_valid_one_hot :
  j = 1
  Y_B_valid = np.append(Y_B_valid, np.array(i[j]))

# Y_C_valid에 세번째 행만 넣을 것이다.
for i in  Y_valid_one_hot :
  j = 2
  Y_C_valid = np.append(Y_C_valid, np.array(i[j]))
```

for문을 사용해서 Y_test_one_hot에서 각각의 열을 변수에 따로 넣는다.


```python
# Y_A_testn에 첫번째 행만 넣을 것이다.
for i in Y_test_one_hot :
  j = 0
  Y_A_test = np.append(Y_A_test, np.array(i[j]))

# Y_B_test에 두번째 행만 넣을 것이다.
for i in Y_test_one_hot :
  j = 1
  Y_B_test = np.append(Y_B_test, np.array(i[j]))

# Y_C_test에 세번째 행만 넣을 것이다.
for i in  Y_test_one_hot :
  j = 2
  Y_C_test = np.append(Y_C_test, np.array(i[j]))
```

각각의 데이터를 사용하기 위하여 reshape를 사용하여 배열의 형태를 맞춰준다.


```python
Y_A_train= Y_A_train.reshape(90,1)
Y_B_train= Y_B_train.reshape(90,1)
Y_C_train= Y_C_train.reshape(90,1)

Y_A_valid= Y_A_valid.reshape(30,1)
Y_B_valid= Y_B_valid.reshape(30,1)
Y_C_valid= Y_C_valid.reshape(30,1)

y_valid = np.reshape(y_valid,(30,1))
```

## 5. 일대다 방식 로지스틱 회귀 구현

### 5-1. 시그모이드 함수 정의


```python
def logistic(x):
    return 1.0 / (1.0 + np.exp(-x))
```

### 5-2. 경사하강법


```python
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
```

각각 꽃들의 Theta값을 따로 구한다.

* Theta_A : Iris-Setosa(세토사)
* Theta_B : Iris-Versicolor(버시컬러)
* Theta_C : Iris-Virginica(버지니카) 


```python
Theta_A = np.random.randn(n_inputs, 1) # n_inputs -> 특성, n_outputs -> 클래스
Theta_B = np.random.randn(n_inputs, 1)
Theta_C = np.random.randn(n_inputs, 1)
```

#### 5-2-1. Y_A 경사하강법 이용하여 세타값 구하기

Y_train_one_hot 대신 Y_A값 사용


```python
#  배치 경사하강법 구현
eta = 0.01 # 학습률
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta_A)
    Y_proba = logistic(logits)            # 양성확률
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -1/m*(np.sum(Y_A_train * np.log(Y_proba + epsilon) + (1 - Y_A_train ) * np.log(1 - Y_proba + epsilon)))
        print(iteration, loss)       
        
    # Y_proba = np.where(Y_proba>=0.5,1,0)      # 시그모이드 함수를 사용하여 0.5보다 크면 1, 작으면 0으로 data 값 변경
    error = Y_proba - Y_A_train      #  Y_proba -> 0과1, error -> 0 or -1 
    gradients = 1/m * X_train.T.dot(error) # -> 세타값 수정,  T -> 전치행렬
    
    
    Theta_A = Theta_A - eta * gradients      
```

    0 0.4375778183535419
    500 0.31431995535530344
    1000 0.24739460329867496
    1500 0.2015867449720451
    2000 0.16903839269298135
    2500 0.14505018722266635
    3000 0.12678834326159452
    3500 0.11249465278438289
    4000 0.10104061337268973
    4500 0.09167722680238735
    5000 0.08389173872084243
    

#### 5-2-2. Y_B 경사하강법 이용하여 세타값 구하기

Y_train_one_hot 대신 Y_B값 사용


```python
#  배치 경사하강법 구현
eta = 0.01 # 학습률
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta_B)
    Y_proba = logistic(logits)            # 양성확률
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -1/m*(np.sum(Y_B_train * np.log(Y_proba + epsilon) + (1 - Y_B_train ) * np.log(1 - Y_proba + epsilon)))
        print(iteration, loss)       
        
    # Y_proba = np.where(Y_proba>=0.5,1,0)      # 시그모이드 함수를 사용하여 0.5보다 크면 1, 작으면 0으로 data 값 변경
    error = Y_proba - Y_B_train      #  Y_proba -> 0과1, error -> 0 or -1 
    gradients = 1/m * X_train.T.dot(error) # -> 세타값 수정,  T -> 전치행렬
    
    
    Theta_B = Theta_B - eta * gradients    
```

    0 1.2606929830898994
    500 0.6772281840254408
    1000 0.6495112807463842
    1500 0.6321202145095705
    2000 0.6208586756639016
    2500 0.61327529751536
    3000 0.6079464920366938
    3500 0.604036172333173
    4000 0.6010441944343213
    4500 0.5986650864781039
    5000 0.5967082592695107
    

#### 5-2-3. Y_C 경사하강법 이용하여 세타값 구하기

Y_train_one_hot 대신 Y_C값 사용


```python
#  배치 경사하강법 구현
eta = 0.01 # 학습률
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta_C)
    Y_proba = logistic(logits)            # 양성확률
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -1/m*(np.sum(Y_C_train * np.log(Y_proba + epsilon) + (1 - Y_C_train ) * np.log(1 - Y_proba + epsilon)))
        print(iteration, loss)       
        
    # Y_proba = np.where(Y_proba>=0.5,1,0)      # 시그모이드 함수를 사용하여 0.5보다 크면 1, 작으면 0으로 data 값 변경
    error = Y_proba - Y_C_train      #  Y_proba -> 0과1, error -> 0 or -1 
    gradients = 1/m * X_train.T.dot(error) # -> 세타값 수정,  T -> 전치행렬
    
    
    Theta_C = Theta_C - eta * gradients    
```

    0 1.24300345889263
    500 0.7225341776177694
    1000 0.5840168541020665
    1500 0.4994640056646242
    2000 0.4444430012538986
    2500 0.4061493368052233
    3000 0.3779307885022022
    3500 0.3561651077443095
    4000 0.3387606059745092
    4500 0.32443981789478005
    5000 0.31238297396779313
    

세가지 꽃들을 경사하강법을 사용한 Theta값을 구하여 90행 3열의 형태로 하나로 합침

각각의 Theta값을 사용하여 y_predict을 만든다.

y_predict을 사용하여 y_valid와 비교하여 정확도를 계산한다.


```python
logits_A = X_valid.dot(Theta_A)              
Y_proba_A = logistic(logits_A)

logits_B = X_valid.dot(Theta_B)              
Y_proba_B = logistic(logits_B)

logits_C = X_valid.dot(Theta_C)              
Y_proba_C = logistic(logits_C)

Y_proba = np.hstack((Y_proba_A,Y_proba_B,Y_proba_C))      # 각각의 Y_proba를 하나의 Y_proba로 합친다.
y_predict = np.argmax(Y_proba, axis=1) # 가장 높은 확률을 갖는 클래스 선택

y_predict = np.reshape(y_predict,(30,1))

accuracy_score = np.mean(y_predict == y_valid) # 정확도 계산
accuracy_score
```




    0.8666666666666667



## 6. 규제가 추가된 경사하강법 활용 훈련

규제가 추가된 경사하강법을 활용한 Theta값을 구하여 90행 3열의 형태로 하나로 합침

각각의 Theta값을 사용하여 y_predict을 만든다.

y_predict을 사용하여 y_valid와 비교하여 정확도를 계산한다.

### 6-1. Y_A 규제 추가된 경사하강법 활용

손실함수를 사용하여 규제 추가된 경사하강법을 활용할 수 있다.


```python
eta = 0.08
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta_A = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta_A)
    Y_proba = logistic(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -1/m*(np.sum(Y_A_train * np.log(Y_proba + epsilon) + (1 - Y_A_train ) * np.log(1 - Y_proba + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta_A[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba -Y_A_train
    l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta_A[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta_A = Theta_A - eta * gradients
```

    0 3.1352627662530685
    500 0.19750727324399006
    1000 0.18458393190013694
    1500 0.18281940095844817
    2000 0.18252746573662926
    2500 0.18247550285041686
    3000 0.182465972642171
    3500 0.18246420250557344
    4000 0.18246387193649002
    4500 0.18246381006055457
    5000 0.1824637984676184
    

### 6-2. Y_B 규제 추가된 경사하강법 활용


```python
eta = 0.08
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta_B = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta_B)
    Y_proba = logistic(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -1/m*(np.sum(Y_B_train * np.log(Y_proba + epsilon) + (1 - Y_B_train ) * np.log(1 - Y_proba + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta_B[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba -Y_B_train
    l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta_B[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta_B = Theta_B - eta * gradients
```

    0 1.7143076904453414
    500 0.6161779468312925
    1000 0.6070114879266042
    1500 0.6065546142771823
    2000 0.6065281499847942
    2500 0.6065265601288568
    3000 0.6065264637577777
    3500 0.6065264579022768
    4000 0.6065264575460534
    4500 0.6065264575243177
    5000 0.6065264575229762
    

### 6-3. Y_C 규제 추가된 경사하강법 활용


```python
eta = 0.08
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta_C = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta_C)
    Y_proba = logistic(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -1/m*(np.sum(Y_C_train * np.log(Y_proba + epsilon) + (1 - Y_C_train ) * np.log(1 - Y_proba + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta_C[1:]))  # 편향은 규제에서 제외C
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba -Y_C_train
    l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta_C[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta_C = Theta_C - eta * gradients
```

    0 3.8854347934040043
    500 0.3916315460644847
    1000 0.35154136014068504
    1500 0.3394978854752998
    2000 0.3346902658661324
    2500 0.33252340237374123
    3000 0.33148207794391515
    3500 0.3309622809353726
    4000 0.3306964867467214
    4500 0.33055838537542803
    5000 0.3304858426427163
    


```python
logits_A = X_valid.dot(Theta_A)              
Y_proba_A = logistic(logits_A)

logits_B = X_valid.dot(Theta_B)              
Y_proba_B = logistic(logits_B)

logits_C = X_valid.dot(Theta_C)              
Y_proba_C = logistic(logits_C)

Y_proba = np.hstack((Y_proba_A,Y_proba_B,Y_proba_C))      # 각각의 Y_proba를 하나의 Y_proba로 합친다.
y_predict = np.argmax(Y_proba, axis=1) # 가장 높은 확률을 갖는 클래스 선택

y_predict = np.reshape(y_predict,(30,1))

accuracy_score = np.mean(y_predict == y_valid) # 정확도 계산
accuracy_score
```




    0.8333333333333334



## 7. 조기 종료 추가
* 모델의 훈련 세트에 대한 과대 적합 방지를 위해 훈련을 적절한 시기에 중단


조기 종료가 추가된 경사하강법을 활용한 Theta값을 구하여 90행 3열의 형태로 하나로 합침

각각의 Theta값을 사용하여 y_predict을 만든다.

y_predict을 사용하여 y_valid와 비교하여 정확도를 계산한다.

### 7-1. Y_A 조기 종료 추가 


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta_A = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta_A)
    Y_proba = logistic(logits)
    error = Y_proba - Y_A_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, 1]), alpha * Theta_A[1:]]
    Theta_A = Theta_A - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta_A)
    Y_proba = logistic(logits)
    xentropy_loss = -1/m*(np.sum(Y_A_valid * np.log(Y_proba + epsilon) + (1 - Y_A_valid ) * np.log(1 - Y_proba + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta_A[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 0.3995165833930832
    219 0.10362511575092215
    220 0.1036252082368875 조기 종료!
    

### 7-2. Y_A 조기 종료 추가 


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta_B = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta_B)
    Y_proba = logistic(logits)
    error = Y_proba - Y_B_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, 1]), alpha * Theta_B[1:]]
    Theta_B = Theta_B - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta_B)
    Y_proba = logistic(logits)
    xentropy_loss = -1/m*(np.sum(Y_B_valid * np.log(Y_proba + epsilon) + (1 - Y_B_valid ) * np.log(1 - Y_proba + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta_B[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 0.37326418539764344
    324 0.21600122064295363
    325 0.21600130976429946 조기 종료!
    

### 7-3. Y_C 조기 종료 추가 


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta_C = np.random.randn(n_inputs, 1)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta_C)
    Y_proba = logistic(logits)
    error = Y_proba - Y_C_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, 1]), alpha * Theta_C[1:]]
    Theta_C = Theta_C - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta_C)
    Y_proba = logistic(logits)
    xentropy_loss = -1/m*(np.sum(Y_C_valid * np.log(Y_proba + epsilon) + (1 - Y_C_valid ) * np.log(1 - Y_proba + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta_C[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss) 
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 0.5103201605141204
    500 0.14695352666173356
    678 0.14564268720871207
    679 0.14564273301583935 조기 종료!
    


```python
logits_A = X_valid.dot(Theta_A)              
Y_proba_A = logistic(logits_A)

logits_B = X_valid.dot(Theta_B)              
Y_proba_B = logistic(logits_B)

logits_C = X_valid.dot(Theta_C)              
Y_proba_C = logistic(logits_C)

Y_proba = np.hstack((Y_proba_A,Y_proba_B,Y_proba_C))      # 각각의 Y_proba를 하나의 Y_proba로 합친다.
y_predict = np.argmax(Y_proba, axis=1) # 가장 높은 확률을 갖는 클래스 선택

y_predict = np.reshape(y_predict,(30,1))

accuracy_score = np.mean(y_predict == y_valid) # 정확도 계산
accuracy_score
```




    0.8



## 8. 테스트 세트 평가

마지막으로 y_predict와 y_test를 비교하여 정확도를 계산한다.


```python
logits_A = X_test.dot(Theta_A)              
Y_proba_A = logistic(logits_A)

logits_B = X_test.dot(Theta_B)              
Y_proba_B = logistic(logits_B)

logits_C = X_test.dot(Theta_C)              
Y_proba_C = logistic(logits_C)

Y_proba = np.hstack((Y_proba_A,Y_proba_B,Y_proba_C))      # 각각의 Y_proba를 하나의 Y_proba로 합친다.
y_predict = np.argmax(Y_proba, axis=1) # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_test) # 정확도 계산
accuracy_score
```




    0.8333333333333334


